"use strict";(self.webpackChunknjit_gsa=self.webpackChunknjit_gsa||[]).push([[6845],{3905:function(e,t,n){n.d(t,{Zo:function(){return u},kt:function(){return m}});var r=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function c(e,t){if(null==e)return{};var n,r,o=function(e,t){if(null==e)return{};var n,r,o={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=r.createContext({}),l=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},u=function(e){var t=l(e.components);return r.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},p=r.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,s=e.parentName,u=c(e,["components","mdxType","originalType","parentName"]),p=l(n),m=o,f=p["".concat(s,".").concat(m)]||p[m]||d[m]||i;return n?r.createElement(f,a(a({ref:t},u),{},{components:n})):r.createElement(f,a({ref:t},u))}));function m(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,a=new Array(i);a[0]=p;var c={};for(var s in t)hasOwnProperty.call(t,s)&&(c[s]=t[s]);c.originalType=e,c.mdxType="string"==typeof e?e:o,a[1]=c;for(var l=2;l<i;l++)a[l]=n[l];return r.createElement.apply(null,a)}return r.createElement.apply(null,n)}p.displayName="MDXCreateElement"},7389:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return c},contentTitle:function(){return s},metadata:function(){return l},toc:function(){return u},default:function(){return p}});var r=n(7462),o=n(3366),i=(n(7294),n(3905)),a=["components"],c={sidebar_position:35},s="034",l={unversionedId:"research-day-2019/034",id:"research-day-2019/034",isDocsHomePage:!1,title:"034",description:"Title:",source:"@site/docs/research-day-2019/034.md",sourceDirName:"research-day-2019",slug:"/research-day-2019/034",permalink:"/docs/research-day-2019/034",tags:[],version:"current",sidebarPosition:35,frontMatter:{sidebar_position:35},sidebar:"defaultSidebar",previous:{title:"033",permalink:"/docs/research-day-2019/033"},next:{title:"035",permalink:"/docs/research-day-2019/035"}},u=[{value:"Title:",id:"title",children:[]},{value:"Presenter:",id:"presenter",children:[]},{value:"Abstract:",id:"abstract",children:[]},{value:"Author(s):",id:"authors",children:[]},{value:"Funding Acknowledgements:",id:"funding-acknowledgements",children:[]}],d={toc:u};function p(e){var t=e.components,n=(0,o.Z)(e,a);return(0,i.kt)("wrapper",(0,r.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"034"},"034"),(0,i.kt)("h2",{id:"title"},"Title:"),(0,i.kt)("p",null,"Joint Learning for Pneumonia Classification and Segmentation on Medical Images"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Discipline:")," Computer Science"),(0,i.kt)("h2",{id:"presenter"},"Presenter:"),(0,i.kt)("p",null,"Shaobo Liu"),(0,i.kt)("h2",{id:"abstract"},"Abstract:"),(0,i.kt)("p",null,"Chest X-ray images are notoriously difficult to analyze due to its noisy nature. Automatic identification of pneumonia on medical images has attracted intensive study recently. In this paper, a joint-task model that simultaneously learns pneumonia classification and segmentation is presented. Two modules are proposed, the image preprocessing module and the attention module, and then combined with the well-known VGG 16. Experimental results performed on the massive dataset of RSNA (Radiology Society of North America) have confirmed the efficiency. By define the data samples to be healthy and pneumonia, the classification model\u2019s test accuracy is improved from 0.89(the baseline model) to 0.93(improved) model. The segmentation model achieves a mean precision result of 0.58 (for the baseline model) to 0.78 (improved model). Furthermore, two weakly supervised learning methods: class-saliency map and grad-cam are used to interpret and refine corresponding area when classification model makes its prediction. Experiment results shows our improved model could focus on the correct area with a high confidence. "),(0,i.kt)("h2",{id:"authors"},"Author(s):"),(0,i.kt)("p",null,"Shaobo Liu, Xin Zhong, Frank Shih"),(0,i.kt)("h2",{id:"funding-acknowledgements"},"Funding Acknowledgements:"),(0,i.kt)("p",null,"nan"))}p.isMDXComponent=!0}}]);